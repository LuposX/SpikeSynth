#  Phase 1: topology + neuron dynamics. Keep optimizer and scheduler fixed.

method: random
name: "NetTopologySweep"
metric:
  name: val_loss
  goal: minimize
parameters:
  lr:
    value: 0.005
  num_hidden:
    values: [4, 8, 16, 32, 64, 128, 256]
  # Defines the Decay of the LIF Neurons
  beta:
    values: [0.3, 0.5, 0.8]
  surrogate_gradient:
      value: "atan"
  # Slope of the surrogate gradient function
  alpha:
      values: [2, 4, 6, 8]
  dropout:
      value: 0
  batch_size:
    value: 2048
  optimizer_class:
    value: "AdamW"
  num_hidden_layers:
      values: [2, 4, 8, 16]
  temporal_skip:
      value: -1
  use_bntt:
      value: False
  use_layernorm:
      value: False
  bntt_time_steps:
      value: 100
  layer_skip:
      value: 0
  scheduler_class:
    value: "cosine" # ["cosine", "exponential", "step", "plateau"]
  scheduler_kwargs:
    value: ""
  data_path:
      value: "./data/small_dataset.ds"
  epochs:
    value: 40
run_cap: 50
