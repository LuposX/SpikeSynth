#!/usr/bin/env python

import snntorch as snn
from snntorch._layers.bntt import BatchNormTT1d

import pytorch_lightning as L
import torch
import torch.nn as nn

import re


class SpikeSynth(L.LightningModule):
    def __init__(self,
                 num_hidden_layers,
                 num_hidden,
                 beta,
                 optimizer_class,
                 optimizer_kwargs,
                 lr,
                 batch_size,
                 dropout,
                 surrogate_gradient,
                 max_epochs,
                 temporal_skip,
                 layer_skip,
                 use_bntt,
                 scheduler_class,
                 log_every_n_steps,
                 use_layernorm,
                 neuron_type="LeakyParallel",
                 scheduler_kwargs=None,
                 bntt_time_steps=None,
                 train_dataset=None,
                 valid_dataset=None,
                 ):
        """
        Initializes the SpikeSynth model, a spiking neural network (SNN). 
        The model supports temporal and layer-wise skip connections,
        customizable surrogate gradients, and flexible optimizer/lr scheduling.

        Args:
            num_hidden_layers (int): 
                Number of recurrent Leaky Integrate-and-Fire (LIF) layers in the network.

            num_hidden (int): 
                Number of hidden neurons (units) in each LIF layer.

            beta (float): 
                Membrane potential decay constant for the LIF neurons (typically 0 < beta < 1).

            optimizer_class (type): 
                Optimizer class to use (e.g., `torch.optim.Adam`, `torch.optim.SGD`).

            neuron_type: str, one of {"LeakyParallel", "Leaky", "SLSTM"}
               "LeakyParallel" uses snn.LeakyParallel and accepts a full (T,B,F) sequence. 
               "Leaky" uses snn.Leaky and is stepped through time manually (per-time-step). SLSTM" uses snn.SLSTM (the existing code path).

            optimizer_kwargs(dict):
                The paramaters of the optimizer.

            lr (float): 
                Inital Learning rate for the optimizer.

            batch_size (int): 
                Batch size used for both training and validation DataLoaders.

            dropout (float):
                Value between [0, 1] if non zero, introduces a dropout layer.

            surrogate_gradient (torch.autograd.Function): 
                Surrogate gradient function for backpropagation through spikes
                (e.g., `snn.surrogate.atan()`).

            max_epochs (int): 
                Maximum number of training epochs. Used for scheduling when employing
                cosine annealing or similar schedulers.

            temporal_skip (int or -1): 
                Temporal skip interval in time steps for residual connections within
                each LIF layer. If `-1`, no temporal skip is applied.

            layer_skip (int or 0): 
                Number of layers to skip for inter-layer residual connections.
                For example, `layer_skip=1` adds a connection from each layer
                to the one immediately above it. If `0`, no layer skip is applied.

            use_bntt (bool):
                Whether to use 1D Batch Normalization Layer with length time_steps. 
                Uses Batch Normalisation Through Time (BNTT).

            bntt_time_steps (None or int):
                Batch Normalization Layer requries the amount of time steps that will be used i.e. sequecne length.

            train_dataset (torch.utils.data.Dataset, optional): 
                Dataset used for training. Required by `train_dataloader()`.

            valid_dataset (torch.utils.data.Dataset, optional): 
                Dataset used for validation. Required by `val_dataloader()`.

            scheduler_class (torch.optim.lr_scheduler):
                What Learning Rate Scheduler should be used for training.

            use_slstm (bool): 
                If True, use snn.SLSTM layers for the recurrent layers.

            log_every_n_steps (int):
                Log every n steps.

            use_layernorm (bool):
                If layernorm should be used afetr every Lif layer.
        """
        super().__init__()

        self.num_params = 6
        self.num_inputs = 1
        self.num_outputs = self.num_inputs
        self.train_dataset = train_dataset
        self.valid_dataset = valid_dataset
        self.max_epochs = max_epochs
        self.surrogate_gradient = surrogate_gradient

        self.save_hyperparameters(ignore=["surrogate_gradient", "train_dataset", "valid_dataset"])

        if neuron_type == "SLSTM" and dropout != 0:
            raise ValueError("SLSTM doesn't support dropout.")

        self.norm = nn.LayerNorm(self.num_inputs + self.num_params)
        self.lif_layers = nn.ModuleList()
        self.leaky_linears = nn.ModuleList()
        self.temp_skip_projs = nn.ModuleList()
        self.layer_skip_projs = nn.ModuleList()
        self.layer_bntt = nn.ModuleList()
        self.layer_norms = nn.ModuleList()

        self._build_layers(neuron_type, bntt_time_steps)

        self.output_layer = nn.Linear(self.hparams.num_hidden, self.num_outputs)

    def _build_layers(self, neuron_type, bntt_time_steps):
        input_size = self.num_inputs + self.num_params
        for i in range(self.hparams.num_hidden_layers):
            if neuron_type == "SLSTM":
                layer = snn.SLSTM(
                    input_size=input_size,
                    hidden_size=self.hparams.num_hidden,
                    spike_grad=self.surrogate_gradient
                )
            elif neuron_type == "LeakyParallel":
                layer = snn.LeakyParallel(
                    input_size=input_size,
                    hidden_size=self.hparams.num_hidden,
                    beta=self.hparams.beta,
                    spike_grad=self.surrogate_gradient,
                    dropout=self.hparams.dropout
                )
            elif neuron_type == "Leaky":
                layer = snn.Leaky(
                    beta=self.hparams.beta,
                    spike_grad=self.surrogate_gradient
                )
                if i == 0:
                    self.leaky_linears.append(nn.Linear(input_size, self.hparams.num_hidden))
                else:
                    self.leaky_linears.append(nn.Linear(self.hparams.num_hidden, self.hparams.num_hidden))
            elif neuron_type == "RLeaky":
                layer = snn.RLeaky(
                    beta=self.hparams.beta,
                    linear_features=self.hparams.num_hidden,
                    spike_grad=self.surrogate_gradient,
                    learn_recurrent=True
                )
                if i == 0:
                    self.leaky_linears.append(nn.Linear(input_size, self.hparams.num_hidden))
                else:
                    self.leaky_linears.append(nn.Linear(self.hparams.num_hidden, self.hparams.num_hidden))
            else:
                raise ValueError(f"Unknown neuron_type: {neuron_type}")

            self.lif_layers.append(layer)

            if input_size != self.hparams.num_hidden and self.hparams.temporal_skip != -1:
                self.temp_skip_projs.append(nn.Linear(input_size, self.hparams.num_hidden))
            else:
                self.temp_skip_projs.append(nn.Identity())

            if self.hparams.layer_skip > 0 and i >= self.hparams.layer_skip:
                self.layer_skip_projs.append(nn.Linear(self.hparams.num_hidden, self.hparams.num_hidden))
            else:
                self.layer_skip_projs.append(None)

            if self.hparams.use_bntt:
                if bntt_time_steps is None and self.hparams.bntt_time_steps is None:
                    raise ValueError("bntt_time_steps must be specified if use_bntt=True")
                time_steps = self.hparams.bntt_time_steps or bntt_time_steps
                self.layer_bntt.append(BatchNormTT1d(self.hparams.num_hidden, time_steps))
            else:
                self.layer_bntt.append(None)

            if self.hparams.use_layernorm:
                self.layer_norms.append(nn.LayerNorm(self.hparams.num_hidden))
            else:
                self.layer_norms.append(None)

            input_size = self.hparams.num_hidden

    def _norm_and_bntt(self, idx, tensor, t=None, per_timestep=False):
        """
        Apply LayerNorm (per-feature) and BNTT if present for layer idx.
        """
        ln = self.layer_norms[idx]
        bntt = self.layer_bntt[idx]
        expected_feat = self.hparams.num_hidden

        # LayerNorm
        if ln is not None and tensor.shape[-1] == ln.normalized_shape[0]:
            if per_timestep:
                tensor = ln(tensor)
            else:
                # (T, B, F) -> (B, T, F) for LayerNorm across features
                tensor = tensor.permute(1, 0, 2)  # (B, T, F)
                tensor = ln(tensor)
                tensor = tensor.permute(1, 0, 2)  # (T, B, F)
        else:
            # If feature size doesn't match, skip LayerNorm for safety.
            # (This commonly happens for the first layer where input_size != num_hidden.)
            pass

        # BNTT
        if bntt is not None and tensor.shape[-1] == expected_feat:
            if per_timestep:
                if t is None:
                    raise ValueError("time index t required for per-timestep BNTT")
                tensor = bntt[t](tensor)
            else:
                normed = []
                for tt in range(tensor.shape[0]):  # iterate T
                    normed.append(bntt[tt](tensor[tt]))
                tensor = torch.stack(normed, dim=0)
        else:
            # If the feature-size doesn't match the BNTT expected features, skip it.
            pass

        return tensor

    def _temporal_proj(self, idx, t, x_seq, raw_input_t=None):
        skip_k = self.hparams.temporal_skip
        if skip_k == -1 or t < skip_k:
            proj = None
        else:
            prev = x_seq[t - skip_k]
            proj = self.temp_skip_projs[idx](prev)
        return proj

    def _layer_skip_proj(self, idx, i, t, track_layer_time, layer_outputs_time):
        if self.hparams.layer_skip <= 0 or i < self.hparams.layer_skip:
            return None

        if track_layer_time:
            skip_layer_out = layer_outputs_time[i - self.hparams.layer_skip][t]
        else:
            skip_layer_out = None

        if skip_layer_out is None:
            return None

        proj_mod = self.layer_skip_projs[i]
        if proj_mod is not None:
            return proj_mod(skip_layer_out)
        else:
            return skip_layer_out

    def _stack_last_layer_seq(self, layer_outputs_time, T, curr_input):
        if layer_outputs_time is not None:
            return torch.stack(layer_outputs_time[-1], dim=0)
        else:
            return curr_input.unsqueeze(0).repeat(T, 1, 1)

    def forward(self, x, params=None):
        self.spike_counts = []

        if x.dim() == 2:
            x = x.unsqueeze(-1)
        elif x.dim() != 3:
            raise ValueError(f"x must be 2D or 3D, got {x.shape}")

        batch, seq_len, _ = x.shape

        if params is not None:
            if params.dim() < 2 or params.shape[1] < self.num_params:
                raise ValueError(f"params must have shape (batch, >= {self.num_params})")
            params_expanded = params.unsqueeze(2)
            x = torch.cat([params_expanded, x], dim=1)
        elif seq_len <= self.num_params:
            raise ValueError(f"x sequence too short to contain static params, got seq_len={seq_len}")

        static_params = x[:, :self.num_params, 0]
        time_series = x[:, self.num_params:, 0]
        time_steps = time_series.shape[1]

        static_repeated = static_params.unsqueeze(1).repeat(1, time_steps, 1)
        time_series_features = time_series.unsqueeze(2)
        x_transformed = torch.cat([static_repeated, time_series_features], dim=2)

        x_seq = x_transformed.permute(1, 0, 2)
        T, B, F_in = x_seq.shape

        skip_k = self.hparams.temporal_skip
        neuron_type = getattr(self.hparams, "neuron_type", "LeakyParallel")

        # ------------------ SLSTM branch ------------------
        if neuron_type == "SLSTM":
            num_layers = len(self.lif_layers)
            syn_states = []
            mem_states = []
            for layer in self.lif_layers:
                syn, mem = layer.reset_mem()
                syn_states.append(syn.to(x_seq.device))
                mem_states.append(mem.to(x_seq.device))

            track_layer_time = (skip_k != -1)
            layer_outputs_time = [ [] for _ in range(num_layers) ] if track_layer_time else None
            spike_acc = [0.0] * num_layers

            for t in range(T):
                curr_input = x_seq[t]  # (B, F_in or F_hidden)
                for i, layer in enumerate(self.lif_layers):
                    # --- ORDER: 1) skipping (temporal & layer), 2) normalization/BNTT, 3) layer call ---
                    input_to_layer = curr_input

                    # temporal skip (adds projected previous input sequence element)
                    prev_proj = self._temporal_proj(i, t, x_seq)
                    if prev_proj is not None:
                        input_to_layer = input_to_layer + prev_proj

                    # layer skip: use earlier layer output from the same time step (if tracking)
                    skip_proj = self._layer_skip_proj(i, i, t, track_layer_time, layer_outputs_time)
                    if skip_proj is not None:
                        input_to_layer = input_to_layer + skip_proj

                    # normalization & BNTT (per-timestep)
                    # Only applied if feature dims match the per-layer norm/BNTT expectations.
                    input_to_layer = self._norm_and_bntt(i, input_to_layer, t=t, per_timestep=True)

                    # now call layer with normalized/skipped input
                    spk, syn_states[i], mem_states[i] = layer(input_to_layer, syn_states[i], mem_states[i])

                    if track_layer_time:
                        layer_outputs_time[i].append(spk)

                    curr_input = spk
                    spike_acc[i] += spk.mean().item()

            last_layer_seq = self._stack_last_layer_seq(layer_outputs_time, T, curr_input)
            self.spike_counts = [c / T for c in spike_acc]
            x_seq = last_layer_seq

        # ------------------ Leaky (per-step) branch ------------------
        elif neuron_type == "Leaky":
            num_layers = len(self.lif_layers)
            mem_states = [layer.init_leaky().to(x_seq.device) for layer in self.lif_layers]
            track_layer_time = (skip_k != -1)
            layer_outputs_time = [[] for _ in range(num_layers)] if track_layer_time else None
            spike_acc = [0.0] * num_layers
            curr_input = None

            for t in range(T):
                raw_input_t = x_seq[t]
                for i, layer in enumerate(self.lif_layers):
                    lin = self.leaky_linears[i]
                    if i == 0:
                        weighted_input = lin(raw_input_t)
                    else:
                        weighted_input = lin(curr_input)

                    prev_proj = self._temporal_proj(i, t, x_seq)
                    if prev_proj is not None:
                        weighted_input = weighted_input + prev_proj

                    skip_proj = self._layer_skip_proj(i, i, t, track_layer_time, layer_outputs_time)
                    if skip_proj is not None:
                        weighted_input = weighted_input + skip_proj

                    weighted_input = self._norm_and_bntt(i, weighted_input, t=t, per_timestep=True)

                    spk, mem_states[i] = layer(weighted_input, mem_states[i])

                    if track_layer_time:
                        layer_outputs_time[i].append(spk)

                    curr_input = spk
                    spike_acc[i] += spk.mean().item()

            last_layer_seq = self._stack_last_layer_seq(layer_outputs_time, T, curr_input)
            self.spike_counts = [c / T for c in spike_acc]
            x_seq = last_layer_seq

        # ------------------ RLeaky (per-step) branch ------------------
        elif neuron_type == "RLeaky":
            num_layers = len(self.lif_layers)
            spk_states = []
            mem_states = []
            for layer in self.lif_layers:
                spk0, mem0 = layer.init_rleaky()
                spk_states.append(spk0.to(x_seq.device))
                mem_states.append(mem0.to(x_seq.device))

            track_layer_time = (skip_k != -1)
            layer_outputs_time = [[] for _ in range(num_layers)] if track_layer_time else None
            spike_acc = [0.0] * num_layers
            curr_input = None

            for t in range(T):
                raw_input_t = x_seq[t]
                for i, layer in enumerate(self.lif_layers):
                    lin = self.leaky_linears[i]
                    if i == 0:
                        weighted_input = lin(raw_input_t)
                    else:
                        weighted_input = lin(curr_input)

                    prev_proj = self._temporal_proj(i, t, x_seq)
                    if prev_proj is not None:
                        weighted_input = weighted_input + prev_proj

                    skip_proj = self._layer_skip_proj(i, i, t, track_layer_time, layer_outputs_time)
                    if skip_proj is not None:
                        weighted_input = weighted_input + skip_proj

                    weighted_input = self._norm_and_bntt(i, weighted_input, t=t, per_timestep=True)

                    spk, mem_states[i] = layer(weighted_input, spk_states[i], mem_states[i])
                    spk_states[i] = spk

                    if track_layer_time:
                        layer_outputs_time[i].append(spk)

                    curr_input = spk
                    spike_acc[i] += spk.mean().item()

            last_layer_seq = self._stack_last_layer_seq(layer_outputs_time, T, curr_input)
            self.spike_counts = [c / T for c in spike_acc]
            x_seq = last_layer_seq

        # ------------------ LeakyParallel / default sequence-processing neuron ------------------
        else:
            layer_outputs = []
            for i, lif in enumerate(self.lif_layers):
                # --- ORDER: 1) compute temporal + layer skip on input sequence, 2) normalize, 3) feed into lif ---
                input_seq = x_seq  # (T, B, F_in or F_hidden)

                # temporal skip projection (sequence-level)
                if skip_k == -1 or skip_k >= T:
                    prev_proj = None
                else:
                    prev = torch.zeros_like(x_seq)
                    prev[skip_k:] = x_seq[:-skip_k]
                    prev_proj = self.temp_skip_projs[i](prev)

                if prev_proj is not None:
                    input_seq = input_seq + prev_proj

                # layer-wise skip (non-time-tracked): add previous layer's output seq if available
                if self.hparams.layer_skip > 0 and i >= self.hparams.layer_skip:
                    skip_layer_out = layer_outputs[i - self.hparams.layer_skip]
                    if skip_layer_out is not None:
                        if self.layer_skip_projs[i] is not None:
                            skip_proj = self.layer_skip_projs[i](skip_layer_out)
                        else:
                            skip_proj = skip_layer_out
                        input_seq = input_seq + skip_proj

                # Now normalization & BNTT across the sequence BEFORE calling the layer
                input_seq = self._norm_and_bntt(i, input_seq, per_timestep=False)

                # feed into layer
                lif_out = lif(input_seq)  # (T, B, F_hidden)

                # record spikes
                self.spike_counts.append(lif_out.mean())

                # update x_seq for next layer
                x_seq = lif_out

                layer_outputs.append(x_seq)

        # x_seq is now (T, B, F_hidden). Project and return (B, T, outputs) squeezed as original.
        x_seq_b = x_seq.permute(1, 0, 2)  # (B, T, F_hidden)
        out = self.output_layer(x_seq_b).squeeze()
        return out

    def training_step(self, batch, batch_idx):
        X_batch, y_batch = batch
        outputs = self(X_batch)
        loss = torch.nn.MSELoss()(outputs, y_batch.float())
        self.log("train_loss", loss, prog_bar=True, on_step=False, on_epoch=True)

        spike_logs = {
            f"spikes/train_avg_layer_{i}": avg_spikes
            for i, avg_spikes in enumerate(self.spike_counts)
        }
        self.log_dict(spike_logs, on_step=False, on_epoch=True, sync_dist=True)

        # --- replace current gradient-aggregation block with this ---
        grad_means = {}
        for name, param in self.named_parameters():
            if param.grad is None:
                continue
        
            # name examples: "lif_layers.0.rleaky.weight", "leaky_linears.2.weight", "output_layer.weight"
            parts = name.split('.')
        
            # If it's one of our ModuleLists, try to keep the index (e.g. 'leaky_linears.2')
            module_lists = {
                "lif_layers",
                "leaky_linears",
                "layer_skip_projs",
                "temp_skip_projs",
                "layer_bntt",
                "layer_norms",
            }
        
            if parts[0] in module_lists and len(parts) > 1 and parts[1].isdigit():
                layer_name = f"{parts[0]}.{parts[1]}"   # e.g. "leaky_linears.2"
            elif parts[0] == "lif_layers" and len(parts) > 1:
                # lif_layers may have deeper nested names (e.g. lif_layers.0.<submodule>),
                # but we still want lif_layers.<index>
                layer_name = ".".join(parts[:2])
            else:
                # fallback to top-level module name for other params (e.g. output_layer)
                layer_name = parts[0]
        
            grad_means.setdefault(layer_name, []).append(param.grad.abs().mean().item())
        
        # average and prefix with a clear metric name
        grad_means = {
            f"grad_mean/{layer}": sum(vals) / len(vals)
            for layer, vals in grad_means.items()
        }
        self.log_dict(grad_means, on_step=True, on_epoch=False, sync_dist=False)


        return loss

    def validation_step(self, batch, batch_idx):
        X_batch, y_batch = batch
        outputs = self(X_batch)
        loss = torch.nn.MSELoss()(outputs, y_batch.float())
        self.log("val_loss", loss, prog_bar=True, on_step=False, on_epoch=True)
        return loss

    def on_train_epoch_end(self):
        opt = self.optimizers()
        lr = opt.param_groups[0]["lr"]
        self.log("lr", lr, prog_bar=True, on_step=False, on_epoch=True)

    def configure_optimizers(self):
        optimizer_kwargs = dict(self.hparams.optimizer_kwargs or {})
        optimizer_kwargs.setdefault("lr", self.hparams.lr)

        optimizer = self.hparams.optimizer_class(self.parameters(), **optimizer_kwargs)

        if self.hparams.scheduler_class is not None:
            if self.hparams.scheduler_class == torch.optim.lr_scheduler.CosineAnnealingLR:
                self.hparams.scheduler_kwargs.setdefault("T_max", self.hparams.max_epochs)
            if self.hparams.scheduler_class == torch.optim.lr_scheduler.ExponentialLR:
                self.hparams.scheduler_kwargs.setdefault("gamma", 0.95)
            if self.hparams.scheduler_class == torch.optim.lr_scheduler.StepLR:
                self.hparams.scheduler_kwargs.setdefault("step_size", 5)
            scheduler = self.hparams.scheduler_class(optimizer, **self.hparams.scheduler_kwargs)

            return {
                "optimizer": optimizer,
                "lr_scheduler": scheduler,
                "monitor": "val_loss",
            }
        else:
            return optimizer

    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.hparams.batch_size,
            shuffle=True,
            num_workers=8,
        )

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.valid_dataset,
            batch_size=self.hparams.batch_size,
            shuffle=False,
            num_workers=4,
        )

