{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3797db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as L\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "import wandb\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e769c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snn.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ee1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data in in the shape: trainings samples(28k) * number of time steps (100 + 6) * time dimension(1)\n",
    "# The time steps is voltage over time\n",
    "data = torch.load(f'./data/dataset.ds')\n",
    "\n",
    "print(data.keys())\n",
    "print(data['X_train'].shape)\n",
    "print(data['Y_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8eae049",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['X_train'][1].squeeze().numpy()[:-6] # last 6 are parameters of the simulation i.e. not voltage\n",
    "\n",
    "plt.plot(X_train)\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Voltage\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a5ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tensors\n",
    "X_train, Y_train = data['X_train'], data['Y_train']\n",
    "X_valid, Y_valid = data['X_valid'], data['Y_valid']\n",
    "X_test, Y_test = data['X_test'], data['Y_test']\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "valid_dataset = TensorDataset(X_valid, Y_valid)\n",
    "test_dataset  = TensorDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9816487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeSynth(L.LightningModule):\n",
    "    def __init__(self, num_voltage_steps, num_params, num_inputs, num_hidden, num_outputs,\n",
    "                 beta, optimizer_class, lr, train_dataset, valid_dataset, batch_size, gamma):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save hyperparameters for easy access\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Network layers\n",
    "        self.linear0 = nn.Linear(self.hparams.num_inputs, self.hparams.num_hidden)\n",
    "        self.lif0 = snn.Leaky(beta=self.hparams.beta)\n",
    "        self.linear1 = nn.Linear(self.hparams.num_hidden, self.hparams.num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=self.hparams.beta)\n",
    "        self.linear2 = nn.Linear(self.hparams.num_hidden, self.hparams.num_hidden)\n",
    "        self.lif2 = snn.Leaky(beta=self.hparams.beta)\n",
    "        self.linear3 = nn.Linear(self.hparams.num_hidden + self.hparams.num_params, self.hparams.num_outputs)\n",
    "\n",
    "        # Datasets\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        volt = x[:, :self.hparams.num_voltage_steps, :]\n",
    "        params = x[:, self.hparams.num_voltage_steps:, 0]\n",
    "\n",
    "        mem0 = self.lif0.init_leaky()\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        mem_rec = []\n",
    "        for step in range(self.hparams.num_voltage_steps):\n",
    "            x_step = self.linear0(volt[:, step, :])\n",
    "            spk, mem0 = self.lif0(x_step, mem0)\n",
    "\n",
    "            x_step = self.linear1(spk)\n",
    "            spk, mem1 = self.lif1(x_step, mem1)\n",
    "            \n",
    "            x_step = self.linear2(spk)\n",
    "            spk, mem2 = self.lif2(x_step, mem2)\n",
    "\n",
    "            mem_rec.append(mem2)\n",
    "\n",
    "        mem_final = torch.stack(mem_rec, dim=0)[-1]\n",
    "        combined = torch.cat((mem_final, params), dim=1)\n",
    "        out = self.linear3(combined)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = self(X_batch)\n",
    "        loss = torch.nn.MSELoss()(outputs, y_batch.float())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = self(X_batch)\n",
    "        loss = torch.nn.MSELoss()(outputs, y_batch.float())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        opt = self.optimizers()\n",
    "        lr = opt.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # Log the learning rate to W&B or progress bar\n",
    "        self.log(\"lr\", lr, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.hparams.optimizer_class(self.parameters(), lr=self.hparams.lr)\n",
    "        \n",
    "        #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #    optimizer, mode='min'\n",
    "        #)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=self.hparams.gamma\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41cfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_voltage_steps=100\n",
    "num_params=6\n",
    "num_inputs=1\n",
    "num_hidden=10 \n",
    "num_outputs=100\n",
    "beta=0.9\n",
    "lr=1e-3 # Since we use ReduceLROnPlateau we can start with a higher LR\n",
    "gamma=0.95\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "optimizer_class = torch.optim.Adam\n",
    "\n",
    "experiment_name = \"Debug\"\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f1615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpikeSynth(\n",
    "    num_voltage_steps=num_voltage_steps,\n",
    "    num_params=num_params,\n",
    "    num_inputs=num_inputs,\n",
    "    num_hidden=num_hidden,\n",
    "    num_outputs=num_outputs,\n",
    "    beta=beta,\n",
    "    optimizer_class=optimizer_class,\n",
    "    lr=lr,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma\n",
    ")\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d12d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Artifact source-Spike-Synth-surrogate_Lighning_Pipeline.ipynb>"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20251017_153300-zgjoc64u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lupos/Spike-Synth/runs/zgjoc64u' target=\"_blank\">Debug</a></strong> to <a href='https://wandb.ai/lupos/Spike-Synth' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lupos/Spike-Synth' target=\"_blank\">https://wandb.ai/lupos/Spike-Synth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lupos/Spike-Synth/runs/zgjoc64u' target=\"_blank\">https://wandb.ai/lupos/Spike-Synth/runs/zgjoc64u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a CometLogger instance\n",
    "wandb_logger = WandbLogger(\n",
    "                          log_model=True,\n",
    "                          project=\"Spike-Synth\",\n",
    "                          name=experiment_name,\n",
    "                          )\n",
    "\n",
    "# log gradients and model topology\n",
    "wandb_logger.watch(model)\n",
    "wandb_logger.experiment.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f354a7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf0bec81abc43dea21926e6b0953117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6108c09b246451ea9cb8c6c516fcbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                   | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d250a64416f844f08a15f400ce61ee5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcdc4285bfe4e878826435e36599fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    # fast_dev_run=True,\n",
    "    max_epochs=epochs,\n",
    "    logger=wandb_logger,  \n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model)\n",
    "\n",
    "wandb_logger.experiment.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
